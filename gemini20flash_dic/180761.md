```json
{
  "word": "regularizers",
  "phonetics": "/ˈrɛɡjʊˌleɪzərz/",
  "part_of_speech": "noun",
  "translation": "正则化项",
  "definition": "In the context of machine learning, regularizers are techniques used to prevent overfitting by adding a penalty term to the loss function. This penalty discourages overly complex models, promoting better generalization to unseen data.",
  "example": "L1 and L2 regularization are common regularizers used in linear regression and neural networks.",
  "synonyms": ["penalty terms", "regularization techniques"],
  "antonyms": [],
  "usage": {
    "general_usage": [
      {
        "context": "Linear Regression",
        "example": "In linear regression, L2 regularization (Ridge Regression) adds a penalty proportional to the square of the magnitude of the coefficients to the loss function.",
        "translation": "在线性回归中，L2正则化（岭回归）在损失函数中增加一个与系数大小的平方成正比的惩罚项。"
      },
      {
        "context": "Neural Networks",
        "example": "In neural networks, regularizers like dropout randomly deactivate some neurons during training to prevent over-reliance on specific neurons.",
        "translation": "在神经网络中，像dropout这样的正则化方法在训练期间随机停用一些神经元，以防止过度依赖特定的神经元。"
      },
      {
        "context": "Loss Function Modification",
        "example": "Regularizers modify the loss function to penalize complex models, encouraging simpler models that generalize better.",
        "translation": "正则化项修改损失函数以惩罚复杂模型，从而鼓励泛化能力更强的简单模型。"
      }
    ],
    "types_of_regularizers": [
      {
        "type": "L1 Regularization (Lasso)",
        "description": "Adds a penalty equal to the absolute value of the magnitude of coefficients.",
        "effect": "Leads to sparse models with some coefficients being exactly zero.",
        "translation": "增加一个等于系数大小的绝对值的惩罚项，导致一些系数为零的稀疏模型。"
      },
      {
        "type": "L2 Regularization (Ridge)",
        "description": "Adds a penalty equal to the square of the magnitude of coefficients.",
        "effect": "Shrinks the coefficients towards zero but rarely makes them exactly zero.",
        "translation": "增加一个等于系数大小的平方的惩罚项，使系数向零收缩，但很少使其完全为零。"
      },
      {
        "type": "Elastic Net Regularization",
        "description": "A combination of L1 and L2 regularization.",
        "effect": "Combines the effects of both L1 and L2 regularization.",
        "translation": "L1和L2正则化的组合，结合了两者的效果。"
      },
      {
        "type": "Dropout",
        "description": "Randomly deactivates neurons during training.",
        "effect": "Prevents over-reliance on specific neurons, improving generalization.",
        "translation": "在训练期间随机停用神经元，防止过度依赖特定的神经元，提高泛化能力。"
      },
      {
        "type": "Early Stopping",
        "description": "Stops the training process when performance on a validation set starts to degrade.",
        "effect": "Prevents overfitting by not allowing the model to train until it memorizes the training data.",
        "translation": "当验证集上的性能开始下降时，停止训练过程，通过不允许模型训练到记住训练数据来防止过拟合。"
      }
    ],
    "mathematical_representation": [
      {
        "formula": "Loss = Original Loss + λ * Regularization Term",
        "description": "λ (lambda) is the regularization parameter that controls the strength of the regularization.",
        "translation": "λ（lambda）是正则化参数，用于控制正则化的强度。"
      },
      {
        "example": "For L2 regularization: Loss = Original Loss + λ * Σ(coefficient^2)",
        "translation": "对于L2正则化：损失 = 原始损失 + λ * Σ(系数^2)"
      }
    ]
  },
  "phrases": {
    "examples": [
      {
        "phrase": "Apply regularizers",
        "translation": "应用正则化项"
      },
      {
        "phrase": "Tune regularization parameters",
        "translation": "调整正则化参数"
      }
    ],
    "idioms": []
  }
}
``` 