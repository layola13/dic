```json
{
  "word": "tokenizers",
  "phonetics": "/ˈtoʊkənaɪzərz/",
  "part_of_speech": "noun",
  "translation": "分词器",
  "definition": "A tokenizer is a component that splits a text into smaller units, called tokens. These tokens can be words, subwords, or characters, and are the building blocks for further processing in natural language processing tasks.",
  "example": "Tokenizers are essential for natural language processing because they break down text into manageable units for analysis.",
  "synonyms": ["word splitter", "text segmenter"],
  "antonyms": [],
  "usage": {
    "general": [
      {
        "example": "Different tokenizers use different algorithms to split text, impacting the performance of NLP models.",
        "translation": "不同的分词器使用不同的算法来分割文本，这会影响自然语言处理模型的性能。"
      },
      {
        "example": "The choice of tokenizer depends on the specific task and language of the text.",
        "translation": "分词器的选择取决于具体的任务和文本的语言。"
      }
    ],
    "types": [
      {
        "example": "Word tokenizers split text into individual words.",
        "translation": "单词分词器将文本分割成单独的单词。"
      },
      {
        "example": "Subword tokenizers break words into smaller units, useful for handling rare words and morphological variations.",
        "translation": "子词分词器将单词分解成更小的单元，这对于处理罕见词汇和形态变异很有用。"
      },
      {
        "example": "Character tokenizers split text into individual characters.",
        "translation": "字符分词器将文本分割成单独的字符。"
      }
    ],
    "applications": [
      {
        "example": "Tokenizers are used in machine translation to prepare text for translation models.",
        "translation": "分词器用于机器翻译中，为翻译模型准备文本。"
      },
      {
        "example": "In sentiment analysis, tokenizers help identify the words that contribute to the overall sentiment of a text.",
        "translation": "在情感分析中，分词器帮助识别构成文本整体情感的词汇。"
      },
      {
        "example": "Tokenizers are crucial in information retrieval systems for indexing and searching documents.",
        "translation": "分词器在信息检索系统中对于索引和搜索文档至关重要。"
      }
    ]
  },
  "phrases": {
    "examples": [
      {
        "phrase": "Text tokenization",
        "translation": "文本分词"
      },
      {
        "phrase": "Subword tokenization",
        "translation": "子词分词"
      }
    ],
    "related_terms": [
      {
        "term": "Natural Language Processing (NLP)",
        "translation": "自然语言处理"
      },
      {
        "term": "Token",
        "translation": "词元"
      },
      {
        "term": "Corpus",
        "translation": "语料库"
      }
    ]
  }
}
``` 