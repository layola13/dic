```json
{
  "word": "tokenize",
  "phonetics": "/ˈtoʊkəˌnaɪz/",
  "part_of_speech": "verb",
  "translation": "分词, 词条化",
  "definition": "To break a stream of text into individual words or tokens.",
  "example": "The sentence 'This is an example' can be tokenized into ['This', 'is', 'an', 'example'].",
  "synonyms": ["segment", "parse", "analyze"],
  "antonyms": [],
  "usage": {
    "simple_present": [
      {
        "question": "How do you tokenize a sentence in Python?",
        "translation": "你如何在Python中对一个句子进行分词？"
      },
      {
        "question": "What tool do you use to tokenize text data?",
        "translation": "你使用什么工具来对文本数据进行分词？"
      }
    ],
    "present_continuous": [
      {
        "question": "Why are we tokenizing this document now?",
        "translation": "我们现在为什么要对这份文档进行分词？"
      },
      {
        "question": "What library are you using while tokenizing the text?",
        "translation": "你在对文本进行分词时，使用的是什么库？"
      }
    ],
    "present_perfect": [
      {
        "question": "What methods have you used to tokenize texts?",
        "translation": "你使用过哪些方法来对文本进行分词？"
      },
      {
        "question": "What improvements has tokenizing brought to the process?",
        "translation": "分词为这个过程带来了哪些改进？"
      }
    ],
    "present_perfect_continuous": [
      {
        "question": "How long have you been tokenizing this large dataset?",
        "translation": "你对这个大型数据集进行分词处理有多久了？"
      },
      {
        "question": "What issues have you been facing while tokenizing the text?",
        "translation": "在对文本进行分词时，你遇到了哪些问题？"
      }
    ],
    "simple_past": [
      {
        "question": "How did you tokenize the text in the past?",
        "translation": "过去你是如何对文本进行分词的？"
      },
      {
        "question": "What approach did you use to tokenize the data last year?",
        "translation": "去年你用什么方法来对数据进行分词？"
      }
    ],
    "past_continuous": [
      {
        "question": "What were you doing when you were tokenizing the document?",
        "translation": "当你正在对文档进行分词时，你在做什么？"
      },
      {
        "question": "Which algorithm were you using while tokenizing the text?",
        "translation": "在对文本进行分词时，你使用的是哪种算法？"
      }
    ],
    "past_perfect": [
      {
        "question": "What steps had you taken before you tokenized the text?",
        "translation": "在你对文本进行分词之前，你采取了哪些步骤？"
      },
      {
        "question": "What settings had you configured before you tokenized the document?",
        "translation": "在对文档进行分词之前，你配置了哪些设置？"
      }
    ],
    "past_perfect_continuous": [
      {
        "question": "How long had you been tokenizing the data before the system crashed?",
        "translation": "在系统崩溃之前，你对数据进行分词处理了多久？"
      },
      {
        "question": "What methods had you been trying to tokenize the code?",
        "translation": "你一直在尝试用什么方法来对代码进行分词？"
      }
    ],
    "simple_future": [
      {
        "question": "How will you tokenize the text tomorrow?",
        "translation": "明天你将如何对文本进行分词？"
      },
      {
        "question": "What tool will you use to tokenize the data?",
        "translation": "你将使用什么工具来对数据进行分词？"
      }
    ],
    "future_continuous": [
      {
        "question": "What will you be doing while you are tokenizing the text?",
        "translation": "当你正在对文本进行分词时，你会在做什么？"
      },
      {
        "question": "What algorithm will you be implementing when tokenizing the data?",
        "translation": "在对数据进行分词时，你将实施什么算法？"
      }
    ],
    "future_perfect": [
      {
        "question": "What amount of data will you have tokenized by the end of the day?",
        "translation": "到今天结束时，你将完成多少数据的分词处理？"
      },
      {
        "question": "What changes will you have made once you've tokenized the document?",
        "translation": "一旦你完成了文档的分词处理，你将做出哪些更改？"
      }
    ],
    "future_perfect_continuous": [
      {
        "question": "How long will you have been tokenizing the dataset by next month?",
        "translation": "到下个月，你对数据集进行分词处理有多久了？"
      },
      {
        "question": "What new challenges will you have been facing while tokenizing the data?",
        "translation": "在对数据进行分词时，你将面临哪些新的挑战？"
      }
    ],
    "subjunctive_mood": [
      {
        "example": "If I were to tokenize this string, I would use this library.",
        "translation": "如果要我对这个字符串进行分词，我会使用这个库。"
      },
      {
        "example": "I wish I knew how to properly tokenize this text.",
        "translation": "我希望我知道如何正确地对这段文本进行分词。"
      }
    ],
    "passive_voice": [
      {
        "example": "The text was tokenized using a specific algorithm.",
        "translation": "这段文本使用特定的算法进行了分词处理。"
      },
      {
        "example": "How is the data being tokenized?",
        "translation": "数据是如何进行分词的？"
      }
    ],
    "interrogative_sentences": [
      {
        "question": "How do we tokenize this?",
        "translation": "我们该如何对这个进行分词？"
      },
      {
        "question": "Why tokenize this text?",
        "translation": "为什么要对这段文字进行分词？"
      }
    ],
    "negative_sentences": [
      {
        "example": "I don't know how to tokenize it.",
        "translation": "我不知道如何对它进行分词。"
      },
      {
        "example": "She didn't tokenize the document.",
        "translation": "她没有对文档进行分词。"
      }
    ]
  },
  "phrases": {
    "examples": [
      {
        "phrase": "Tokenize text",
        "translation": "文本分词"
      },
      {
        "phrase": "Data tokenization",
        "translation": "数据分词"
      }
    ],
    "idioms": [
      {
        "phrase": "fine-grained tokenization",
        "translation": "细粒度分词"
      },
      {
        "phrase": "coarse-grained tokenization",
        "translation": "粗粒度分词"
      }
    ]
  }
}
``` 