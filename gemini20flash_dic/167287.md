```json
{
  "word": "pretokened",
  "phonetics": null,
  "part_of_speech": "adj.",
  "translation": "预分词的",
  "definition": "A term used in the context of Natural Language Processing (NLP) to describe text that has already been split into tokens before being processed by a model.",
  "example": "The pretokened text was fed directly into the transformer model.",
  "synonyms": ["preprocessed", "segmented"],
  "antonyms": ["untokenized", "raw"],
  "usage": {
    "general_usage": [
      {
        "example": "Using pretokened data can speed up the training process.",
        "translation": "使用预分词的数据可以加快训练过程。"
      },
      {
        "example": "The model requires the input to be pretokened.",
        "translation": "该模型需要输入为预分词形式。"
      }
    ],
    "technical_context": [
      {
        "example": "The pretokened sequence is then converted into numerical IDs.",
        "translation": "预分词序列随后被转换为数字ID。"
      },
      {
        "example": "We evaluated the model on both raw text and pretokened text.",
        "translation": "我们评估了模型在原始文本和预分词文本上的表现。"
      }
    ],
    "comparative_usage": [
      {
        "example": "Pretokened input reduces the computational overhead compared to raw text input.",
        "translation": "与原始文本输入相比，预分词输入减少了计算开销。"
      },
      {
        "example": "While pretokened text is faster to process, it may lose some contextual information.",
        "translation": "虽然预分词文本处理速度更快，但可能会丢失一些上下文信息。"
      }
    ],
    "specific_algorithms": [
      {
        "example": "For BERT models, the input text is typically pretokened using WordPiece.",
        "translation": "对于BERT模型，输入文本通常使用WordPiece进行预分词。"
      },
      {
        "example": "GPT models often use Byte Pair Encoding (BPE) to generate pretokened sequences.",
        "translation": "GPT模型通常使用字节对编码（BPE）来生成预分词序列。"
      }
    ],
    "data_preprocessing": [
      {
        "example": "The first step in the NLP pipeline is to create pretokened text from the corpus.",
        "translation": "NLP流程的第一步是从语料库中创建预分词文本。"
      },
      {
        "example": "Properly pretokened data ensures better model performance.",
        "translation": "正确的预分词数据确保更好的模型性能。"
      }
    ]
  },
  "phrases": {
    "examples": [
      {
        "phrase": "pretokened data",
        "translation": "预分词数据"
      },
      {
        "phrase": "pretokened input",
        "translation": "预分词输入"
      }
    ],
    "related_terms": [
      {
        "term": "tokenization",
        "translation": "分词"
      },
      {
        "term": "NLP",
        "translation": "自然语言处理"
      },
      {
        "term": "text preprocessing",
        "translation": "文本预处理"
      }
    ]
  }
}
``` 